# Discrimination / Bias

* How does an AI system become bias and therefore discriminated against some people?
* And how do we try to reduce or eliminate this effect in our AI systems?

## AI learning unhealthy stereotypes

A group at Microsoft found this remarkable result that when AI learns from text file on the internet, it can learn unhealthy stereotypes.

<a name= "UnhealthyStereotypes">![Unhealthy Stereotypes](UnhealthyStereotypes.png?raw=true "Unhealthy Stereotypes")
</a>


## Why bias matter

**_For Example:_**

* Hiring tool that discriminates against woman
* Facial recognition working better for specific ethnicity
* Bank loan approvals
* Toxic effect of reinforcing unhealthy stereotypes

## Combating bias

* Technical solution
  * ["Zero out" the bias in words](#UnhealthyStereotypes)
  AI system learns a lot of different numbers with which to store words, there are few numbers that correspond to the bias. If you zero out those numbers, just set them to zero, then the bias diminishes significantly.
  * Use less biased or more inclusive data

